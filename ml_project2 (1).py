# -*- coding: utf-8 -*-
"""ML project2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oKy3h6AYwDcjdMZarztDjNzUMvQh6yXa

Project 2

* Background of Kiva
* Why are loans important?
* Explanatory Data Analysis of loans
* Poverty (MPI)
* What do we do here?

Import Packages
"""

#Import Packages
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

"""Load Files"""

#Load Original Kiva Datasets
loans = pd.read_csv("/Users/kang/Documents/UZH/Semester 4/Machine Learning For Economic Analysis/Projects/Project2/Datasets/kiva_loans.csv")

kiva_mpi_locations_df = pd.read_csv("/Users/kang/Documents/UZH/Semester 4/Machine Learning For Economic Analysis/Projects/Project2/Datasets/kiva_mpi_region_locations.csv")
loan_theme_ids_df = pd.read_csv("/Users/kang/Documents/UZH/Semester 4/Machine Learning For Economic Analysis/Projects/Project2/Datasets/loan_theme_ids.csv")
loan_themes_by_region_df = pd.read_csv("/Users/kang/Documents/UZH/Semester 4/Machine Learning For Economic Analysis/Projects/Project2/Datasets/loan_themes_by_region.csv")

#Load Additional MPI Datasets
#mpi_national_df = pd.read_csv("/content/drive/MyDrive/UZH/Semester 4/ML/Project 2 Datasets/MPI_national.csv")

mpi_subnational = pd.read_csv("/Users/kang/Documents/UZH/Semester 4/Machine Learning For Economic Analysis/Projects/Project2/Datasets/MPI_subnational.csv")

print("Original Kiva Loans dataset: ", loans.shape)

"""Data Preprocessing"""

#Drop Missing Values in MPI dataset
print("Original MPI dataset: ", kiva_mpi_locations_df.shape)
region_mpi_df = kiva_mpi_locations_df[['world_region', 'LocationName', 'country','region', 'MPI', 'lat', 'lon']]
region_mpi_df = region_mpi_df.dropna()
print("Cleaned MPI dataset: ", region_mpi_df.shape)

##Data Preprocessing

#Merging Kiva Loans Dataset to MPI using loan_themes
kiva_loans_mpi_df = pd.merge(loans, loan_theme_ids_df, how='left', on='id')
kiva_loans_mpi_df = kiva_loans_mpi_df.merge(loan_themes_by_region_df, how='left', on=['Partner ID', 'Loan Theme ID', 'country', 'region'])
kiva_loans_mpi_df = kiva_loans_mpi_df.merge(kiva_mpi_locations_df, how='left', left_on=['country', 'mpi_region'], right_on=['country', 'LocationName'])

#Drop rows with missing MPI
kiva_loans_mpi_df = kiva_loans_mpi_df.dropna(subset=['MPI'])

#Remove some information that we no longer need
kiva_loans_mpi_df.drop('mpi_region', axis=1, inplace=True)
kiva_loans_mpi_df.drop('LocationName_y', axis=1, inplace=True)
kiva_loans_mpi_df.drop('sector_y', axis=1, inplace=True)
kiva_loans_mpi_df.drop('Loan Theme Type_y', axis=1, inplace=True)
kiva_loans_mpi_df.drop('ISO_y', axis=1, inplace=True)
kiva_loans_mpi_df.drop('region_y', axis=1, inplace=True)
kiva_loans_mpi_df.drop('geo_y', axis=1, inplace=True)
kiva_loans_mpi_df.drop('lat_y', axis=1, inplace=True)
kiva_loans_mpi_df.drop('lon_y', axis=1, inplace=True)
kiva_loans_mpi_df.drop('tags', axis=1, inplace=True)
kiva_loans_mpi_df.drop('geocode', axis=1, inplace=True)
kiva_loans_mpi_df.drop('geocode_old', axis=1, inplace=True)

# Rename some columns
kiva_loans_mpi_df = kiva_loans_mpi_df.rename(index=str, columns={'region_x': 'region', 'sector_x' : 'sector', 'Loan Theme Type_x':'loan_theme_type',
                                                     'ISO_x':'ISO', 'LocationName_x':'location_name', 'geo_x':'geo', 'lat_x':'lat', 'lon_x':'lon'
                                      })

print("Merged Loans MPI dataset: ", kiva_loans_mpi_df.shape)

#Print head
kiva_loans_mpi_df.head()

#Create New Column "Funded" Column to determine how many loands get funded
kiva_loans_mpi_df["funded"] = ((kiva_loans_mpi_df["loan_amount"] - kiva_loans_mpi_df["funded_amount"]) / (kiva_loans_mpi_df["loan_amount"] - kiva_loans_mpi_df["funded_amount"]) - 1) * -1 
kiva_loans_mpi_df["funded"] = kiva_loans_mpi_df["funded"].astype('bool')
kiva_loans_mpi_df[kiva_loans_mpi_df["funded_amount"] < kiva_loans_mpi_df["loan_amount"]].head()

#To check number of loans that got funded
n_funded = kiva_loans_mpi_df[kiva_loans_mpi_df["funded"] == True ].shape[0]
n_total = kiva_loans_mpi_df.shape[0]

print("{} out of {} loans got funded ({} %)".format(n_funded, n_total, round(n_funded/n_total*100)))

#Transform borrower's gender variable to borrower_female and borrower_male
kiva_loans_mpi_df["borrower_genders"]= kiva_loans_mpi_df["borrower_genders"].astype("str")
kiva_loans_mpi_df["borrower_female"] = kiva_loans_mpi_df["borrower_genders"].astype("str").apply(lambda x: "female" in x)
kiva_loans_mpi_df["borrower_male"] = kiva_loans_mpi_df["borrower_genders"].astype("str").apply(lambda x: len(x) == 4 or (len(x) > 6 and (len(x) % 8) != 6))

#Explore Variables
variables = ["loan_amount", "term_in_months", "repayment_interval","sector", "world_region","lat","lon","rural_pct","MPI","borrower_male", "borrower_female","funded"]
loans_variables = kiva_loans_mpi_df[variables]
loans_variables.head()

#Loan Amount
loans_variables["loan_amount"].describe()

#Term in Months
loans_variables["term_in_months"].describe()

#Repayment Interval
loans_variables["repayment_interval"].describe()

#Sector
loans_variables["sector"].describe()

#World Region
loans_variables["world_region"].describe()

#Latitude
loans_variables["lat"].describe()

#Longtitude
loans_variables["lon"].describe()

#Rural Percentage
loans_variables["rural_pct"].describe()

#MPI
loans_variables["MPI"].describe()

#Shape of Variables
print(loans_variables.shape)
print("{} datapoints".format(loans_variables.shape[0] * loans_variables.shape[1]))

l = loans_variables.copy()

#Create Dummies for Categorical Variables Sector, Repayment Interval & World Region
l = pd.concat([l.drop("sector", axis = 1), pd.get_dummies(l["sector"], prefix = "sector")], axis=1, sort=False)
l = pd.concat([l.drop("repayment_interval", axis = 1), pd.get_dummies(l["repayment_interval"], prefix = "repayment_interval")], axis=1, sort=False)
l = pd.concat([l.drop("world_region", axis = 1), pd.get_dummies(l["world_region"], prefix = "world_region")], axis=1, sort=False)

l.head()

#Convert Boolean Variables to Binary
l["borrower_male"] = l["borrower_male"].astype(int)
l["borrower_female"] = l["borrower_female"].astype(int)
l["funded"] = l["funded"].astype(int)

#l["borrower_male"] = pd.to_numeric(l["borrower_male"])
#l["borrower_female"] = pd.to_numeric(l["borrower_female"])
#l["funded"] = pd.to_numeric(l["funded"])

#Normalize Loan Amount & Term in Month
l["loan_amount_norm"] = l["loan_amount"] / np.max(l["loan_amount"]) # max is 100k
l["term_in_months_norm"] = l["term_in_months"] / np.max(l["term_in_months"]) # max is 158

l = l.drop("loan_amount", axis = 1)
l = l.drop("term_in_months", axis = 1)
l.head()

#Impute missing Values with Median 
l["rural_pct"].fillna(l["rural_pct"].median(),inplace=True)

l["rural_pct"].isna().sum()

l.head()

"""Data Augmentation"""

#As the dataset is heavily unbalanced (where we see that 94% of the loans actually got funded [412555 out of 437985 loans got funded (94 %)]), we will use data augmentation to increase the number of funded datapoints labelled as false. We rely on data augmentation for time and accuracy.
#We augment the data with a factor of 15, which is a proportion of the true labels and false labels. After inflating, we have a total of 819435 data points, which is 1.87 times greater than our previous dataset.
l_false = l[l["funded"] == False]

n_false_labels = l_false.shape[0]
n_true_labels = l.shape[0] - n_false_labels
factor = round(n_true_labels / n_false_labels) - 1
print("inflate all false labels with a factor of {}".format(factor))
false_inflated = pd.concat([l_false] * factor)
l = pd.concat([l, false_inflated])
print("now we have {} data points".format(l.shape[0]))

"""Explanatory Data Analysis"""

#total amount of loans etc???

"""Correlations Among Variables"""

#Check Correlations Among Variables
l_fem = l[l["borrower_female"] == True]

fem_funded = l_fem[l_fem["funded"] == True].shape[0]
fem_tot = l_fem.shape[0]

print("{}/{} females funded ({} %)".format(fem_funded, fem_tot, round(100 * fem_funded/fem_tot)))

l_m = l[l["borrower_male"] == True]

m_funded = l_m[l_m["funded"] == True].shape[0]
m_tot = l_m.shape[0]

print("{}/{} males funded ({} %)".format(m_funded, m_tot, round(100 * m_funded/m_tot)))
#Almost double the females (61%) than males (31%) got funded.

# """Plotting Heatmap"""

# #Plot correlations between MPI and loan factors
# cmap = l.corr()

# alpha = ['ABC', 'DEF', 'GHI', 'JKL']
# alpha = l.columns

# fig_cm = plt.figure(figsize = (15, 15))
# ax = fig_cm.add_subplot(111)
# cax = ax.matshow(cmap, interpolation='nearest', cmap = 'Blues')
# fig_cm.colorbar(cax)

# plt.xticks(np.arange(0, len(l.columns)), rotation='vertical')
# plt.yticks(np.arange(0, len(l.columns)))

# ax.set_xticklabels(['']+alpha)
# ax.set_yticklabels(['']+alpha)
# ax.set_xlabel("Kiva Loan Feature Correlation",fontsize=18)

# plt.show()

# """Observations from heatmap:
# *   Funded is negatively correlated with borrower male (-0.3)
# *   Funded is positively correlated with borrower female (0.31)
# * Funded is weakly negatively correlated with MPI (-0.079)
# * Funded is negatively correlated with repayment interval bullet (-0.18)
# * Funded is positvely correlated with repayment interval irregular (0.33)
# * Funded is negatively correlated with repayment interval monthly (-0.18)
# * Funded is positively correlated with East Asia and Pacific (0.29)
# * Funded is negatively correlated with Latin America and Carribean (-0.14) and Sub-Saharan African (-0.11)
# * Funded is negatively correlated with normalized loan amount (-0.22)
# * Funded is negatively correlated with normalized terms in months (-0.32)


# """

# colmap = sns.diverging_palette(220, 20, sep=20, as_cmap=True)
# plt.figure(figsize = (24,24))
# sns.heatmap(cmap, 
#             xticklabels=cmap.columns.values,
#             yticklabels=cmap.columns.values, 
#             cmap=colmap, vmin=-0.7, vmax=0.7, annot=True, square=True)
# plt.title('Kiva Loan Feature Correlation',fontsize=16)

"""Training Models"""

from sklearn import preprocessing
from sklearn.model_selection import KFold # import KFold
from sklearn.ensemble import RandomForestClassifier #Random Forest
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, plot_confusion_matrix
from sklearn.linear_model import Lasso #Lasso
from sklearn.linear_model import LinearRegression #import LinearRegression
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV #import GridSearchCV
from xgboost import XGBClassifier #import xgboost
from sklearn.ensemble import GradientBoostingClassifier #import GBM
from sklearn.svm import SVC #import SVM


from sklearn.metrics import mean_squared_error
import tensorflow as tf #for NN
#Do OLS, SVM, Light GBM, KNN, xgboost

"""Split Data into Training and Testing with 80-20"""

#Use all variables
all_variables = l.drop("funded", axis = 1).columns

x = l.drop("funded", axis = 1)[all_variables]
y = l["funded"]

#Splitting data into a training set and test set (80-20) using train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,random_state=42)

x_train.shape

x_test.shape

x_train.isna().any().any()
x_train.isna().sum()



# """Train a Linear Regression"""

# #Linear Regression
# linreg = LinearRegression()

# print(linreg)

# linreg_fit = linreg.fit(x_train,y_train)

# linreg_pred = linreg_fit.predict(x_test)

# print(y_test)

# linreg_train_score =linreg.score(x_train,y_train)
# print(linreg_train_score)
# linreg_test_score = linreg.score(x_test, y_test)
# print(linreg_test_score)

# #Training_Accuracy_Before = []
# #Testing_Accuracy_Before = []
# #Accuracy
# #Training_Accuracy_Before.append(linreg_train_score)
# #Testing_Accuracy_Before.append(linreg_test_score)

# #RSME on Testing Data
# mse_linreg = mean_squared_error(y_test, linreg_pred)

# #linreg_mse = mean_squared_error(y_test, linreg_pred)
# print(mse_linreg)


# """Using GridSearchCV and Lasso"""

# ## Performing GridSearchCV with Cross Validation technique on Lasso Regression and finding the optimum value of alpha

# #params = {'alpha': (np.logspace(-8, 8, 100))} # It will check from 1e-08 to 1e+08
# params = {'alpha': [0.0001,0.0005,0.0006,0.06,0.5,0.0001,0.01,1,2,3,4,4.4,4,10]} 
# #params = {'alpha': [1e-10,1e-8,1e-4, 1e-3, 1e-2, 1e-1, 1,5, 10,20]} 
# lasso = Lasso(normalize=True)
# lasso_model = GridSearchCV(lasso, params, cv = 10, scoring='neg_mean_squared_error')
# lasso_model.fit(x_train, y_train)
# #scoring='neg_mean_squared_error',n_jobs=-1
# #find best parameters
# print("Best Params: ",lasso_model.best_params_)
# print("Best Score: ",lasso_model.best_score_)

# #Performance metrics on Testing Data
# pred_grid_lasso = lasso_model.predict(x_test)
# mse_grid_lasso =mean_squared_error(y_test, pred_grid_lasso)
# #y_predict_lasso_grid = pred_grid_lasso.reshape(-1,1)
# print("MSE Lasso:", mse_grid_lasso)

"""Using GridSearchCV and Random Forest"""

param_grid = [
{'n_estimators': [50,200,500], 'max_features': [5, 10], 
 'max_depth': [10, 50, None], 'bootstrap': [True, False],'random_state': [0]}
]

#fit random forest
forest = RandomForestClassifier()

grid_search_forest = GridSearchCV(forest, param_grid, cv=10, scoring='neg_mean_squared_error')
grid_search_forest.fit(x_train, y_train)

print("Best Params: ",grid_search_forest.best_params_)
print("Best Score:", grid_search_forest.best_score_)

pred_grid_rf = grid_search_forest.best_estimator_.predict(x_test)
mse_grid_rf =mean_squared_error(y_test, pred_grid_rf)

y_predict_rf_grid = pred_grid_rf.reshape(-1,1)
print("MSE Random Forest: ", mse_grid_rf)